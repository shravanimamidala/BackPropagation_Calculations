{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEnRwNaA890X",
        "outputId": "40bfeea3-a48e-4adb-ba08-d815655c92ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CONFIGURE YOUR NEURAL NETWORK ===\n",
            "Enter number of input neurons: 1\n",
            "Enter number of hidden layers: 2\n",
            "Enter number of neurons in hidden layer 1: 1\n",
            "Enter number of neurons in hidden layer 2: 1\n",
            "Enter number of output neurons: 3\n",
            "\n",
            "=== SET WEIGHTS FOR EACH CONNECTION ===\n",
            "\n",
            "Input Layer → Hidden Layer 1\n",
            "Weight from Input 1 to Hidden1 Neuron 1: 1\n",
            "\n",
            "Hidden Layer 1 → Hidden Layer 2\n",
            "Weight from Hidden1 Neuron 1 to Hidden2 Neuron 1: 0.5\n",
            "\n",
            "Hidden Layer 2 → Output Layer\n",
            "Weight from Hidden2 Neuron 1 to Output Neuron 1: 1\n",
            "Weight from Hidden2 Neuron 1 to Output Neuron 2: 1.5\n",
            "Weight from Hidden2 Neuron 1 to Output Neuron 3: 1\n",
            "\n",
            "Enter input values (space-separated): 1\n",
            "\n",
            "=== NEURAL NETWORK EXECUTION ===\n",
            "Network Architecture: 1-1-1-3\n",
            "Input Values: [1.0]\n",
            "Using fixed target = 1.0 for backpropagation (ignoring actual output error)\n",
            "\n",
            "=== DETAILED FORWARD PASS ===\n",
            "\n",
            "[Layer 0 → Layer 1]\n",
            "h11: z = (1.00*1.0) = 1.0000\n",
            "      a = σ(1.0000) = 0.7311\n",
            "\n",
            "[Layer 1 → Layer 2]\n",
            "h21: z = (0.50*0.7310585786300049) = 0.3655\n",
            "      a = σ(0.3655) = 0.5904\n",
            "\n",
            "[Hidden2 → Output]\n",
            "y1: z = (1.00*0.5904) = 0.5904\n",
            "     a = σ(0.5904) = 0.6435\n",
            "y2: z = (1.50*0.5904) = 0.8856\n",
            "     a = σ(0.8856) = 0.7080\n",
            "y3: z = (1.00*0.5904) = 0.5904\n",
            "     a = σ(0.5904) = 0.6435\n",
            "\n",
            "=== DETAILED BACKWARD PASS ===\n",
            "\n",
            "[Output Layer]\n",
            "Using fixed target value = 1.0\n",
            "Error = y - target = 0.6435 - 1.0 = -0.3565\n",
            "∂σ/∂z = y*(1-y) = 0.6435*0.3565 = 0.2294\n",
            "δ_output = Error * ∂σ/∂z = -0.3565 * 0.2294 = -0.0818\n",
            "Using fixed target value = 1.0\n",
            "Error = y - target = 0.7080 - 1.0 = -0.2920\n",
            "∂σ/∂z = y*(1-y) = 0.7080*0.2920 = 0.2067\n",
            "δ_output = Error * ∂σ/∂z = -0.2920 * 0.2067 = -0.0604\n",
            "Using fixed target value = 1.0\n",
            "Error = y - target = 0.6435 - 1.0 = -0.3565\n",
            "∂σ/∂z = y*(1-y) = 0.6435*0.3565 = 0.2294\n",
            "δ_output = Error * ∂σ/∂z = -0.3565 * 0.2294 = -0.0818\n",
            "\n",
            "[Hidden2 Gradients]\n",
            "h21:\n",
            "  Weighted sum = -0.254162\n",
            "  ∂σ/∂z = a*(1-a) = 0.5904*0.4096 = 0.2418\n",
            "  δ = Sum * ∂σ/∂z = -0.254162 * 0.2418 = -0.061465\n",
            "\n",
            "[Hidden1 Gradients]\n",
            "h11:\n",
            "  Weighted sum = -0.030732\n",
            "  ∂σ/∂z = a*(1-a) = 0.7311*0.2689 = 0.1966\n",
            "  δ = Sum * ∂σ/∂z = -0.030732 * 0.1966 = -0.006042\n",
            "\n",
            "[Input Gradients]\n",
            "∂E/∂x1 = -0.006042*1.00 = -0.006042\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Traceable Neural Network with Fixed Target Backpropagation\n",
        "import math\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, bias=None):\n",
        "        self.bias = bias\n",
        "        self.weights = []\n",
        "\n",
        "class NeuronLayer:\n",
        "    def __init__(self, num_neurons, bias=None):\n",
        "        self.neurons = [Neuron(bias) for _ in range(num_neurons)]\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self._configure_network()\n",
        "\n",
        "    def _configure_network(self):\n",
        "        print(\"=== CONFIGURE YOUR NEURAL NETWORK ===\")\n",
        "        self.num_inputs = int(input(\"Enter number of input neurons: \"))\n",
        "        self.num_hidden_layers = int(input(\"Enter number of hidden layers: \"))\n",
        "        self.neurons_per_hidden = []\n",
        "        self.hidden_layers = []\n",
        "\n",
        "        for i in range(self.num_hidden_layers):\n",
        "            n = int(input(f\"Enter number of neurons in hidden layer {i + 1}: \"))\n",
        "            self.neurons_per_hidden.append(n)\n",
        "            self.hidden_layers.append(NeuronLayer(n))\n",
        "\n",
        "        self.num_outputs = int(input(\"Enter number of output neurons: \"))\n",
        "        self.output_layer = NeuronLayer(self.num_outputs)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        print(\"\\n=== SET WEIGHTS FOR EACH CONNECTION ===\")\n",
        "\n",
        "        print(f\"\\nInput Layer → Hidden Layer 1\")\n",
        "        for i, neuron in enumerate(self.hidden_layers[0].neurons):\n",
        "            neuron.weights = [\n",
        "                float(input(f\"Weight from Input {j + 1} to Hidden1 Neuron {i + 1}: \"))\n",
        "                for j in range(self.num_inputs)\n",
        "            ]\n",
        "\n",
        "        for l in range(1, self.num_hidden_layers):\n",
        "            print(f\"\\nHidden Layer {l} → Hidden Layer {l + 1}\")\n",
        "            prev_layer = self.hidden_layers[l - 1]\n",
        "            for i, neuron in enumerate(self.hidden_layers[l].neurons):\n",
        "                neuron.weights = [\n",
        "                    float(input(f\"Weight from Hidden{l} Neuron {j + 1} to Hidden{l + 1} Neuron {i + 1}: \"))\n",
        "                    for j in range(len(prev_layer.neurons))\n",
        "                ]\n",
        "\n",
        "        print(f\"\\nHidden Layer {self.num_hidden_layers} → Output Layer\")\n",
        "        for i, neuron in enumerate(self.output_layer.neurons):\n",
        "            neuron.weights = [\n",
        "                float(input(f\"Weight from Hidden{self.num_hidden_layers} Neuron {j + 1} to Output Neuron {i + 1}: \"))\n",
        "                for j in range(len(self.hidden_layers[-1].neurons))\n",
        "            ]\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + math.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, a):\n",
        "        return a * (1 - a)\n",
        "\n",
        "    def full_forward_pass(self, inputs):\n",
        "        print(\"\\n=== DETAILED FORWARD PASS ===\")\n",
        "        activations = [inputs]\n",
        "        for l, layer in enumerate(self.hidden_layers):\n",
        "            prev_activations = activations[-1]\n",
        "            current_outputs = []\n",
        "            print(f\"\\n[Layer {l} → Layer {l+1}]\")\n",
        "            for i, neuron in enumerate(layer.neurons):\n",
        "                z = sum(w * x for w, x in zip(neuron.weights, prev_activations))\n",
        "                a = self.sigmoid(z)\n",
        "                current_outputs.append(a)\n",
        "                print(f\"h{l+1}{i+1}: z = ({' + '.join(f'{w:.2f}*{x}' for w,x in zip(neuron.weights, prev_activations))}) = {z:.4f}\")\n",
        "                print(f\"      a = σ({z:.4f}) = {a:.4f}\")\n",
        "            activations.append(current_outputs)\n",
        "\n",
        "        print(f\"\\n[Hidden{self.num_hidden_layers} → Output]\")\n",
        "        prev_activations = activations[-1]\n",
        "        output_activations = []\n",
        "        for i, neuron in enumerate(self.output_layer.neurons):\n",
        "            z = sum(w * h for w, h in zip(neuron.weights, prev_activations))\n",
        "            a = self.sigmoid(z)\n",
        "            output_activations.append(a)\n",
        "            print(f\"y{i+1}: z = ({' + '.join(f'{w:.2f}*{h:.4f}' for w, h in zip(neuron.weights, prev_activations))}) = {z:.4f}\")\n",
        "            print(f\"     a = σ({z:.4f}) = {a:.4f}\")\n",
        "        activations.append(output_activations)\n",
        "        return activations\n",
        "\n",
        "    def full_backward_pass(self, activations, target):\n",
        "        print(\"\\n=== DETAILED BACKWARD PASS ===\")\n",
        "\n",
        "        output_activations = activations[-1]\n",
        "        errors = [y - target for y in output_activations]\n",
        "        deltas = []\n",
        "        print(\"\\n[Output Layer]\")\n",
        "        for i, (y, err) in enumerate(zip(output_activations, errors)):\n",
        "            dσ_dz = self.sigmoid_derivative(y)\n",
        "            δ = err * dσ_dz\n",
        "            deltas.append(δ)\n",
        "            print(f\"Using fixed target value = {target}\")\n",
        "            print(f\"Error = y - target = {y:.4f} - {target} = {err:.4f}\")\n",
        "            print(f\"∂σ/∂z = y*(1-y) = {y:.4f}*{1-y:.4f} = {dσ_dz:.4f}\")\n",
        "            print(f\"δ_output = Error * ∂σ/∂z = {err:.4f} * {dσ_dz:.4f} = {δ:.4f}\")\n",
        "\n",
        "        layer_deltas = [deltas]\n",
        "        for l in reversed(range(self.num_hidden_layers)):\n",
        "            print(f\"\\n[Hidden{l+1} Gradients]\")\n",
        "            current_layer = self.hidden_layers[l]\n",
        "            next_deltas = layer_deltas[0]\n",
        "            if l == self.num_hidden_layers - 1:\n",
        "                next_layer = self.output_layer\n",
        "            else:\n",
        "                next_layer = self.hidden_layers[l + 1]\n",
        "\n",
        "            layer_delta = []\n",
        "            for i, neuron in enumerate(current_layer.neurons):\n",
        "                weighted_sum = sum(\n",
        "                    next_deltas[j] * next_layer.neurons[j].weights[i]\n",
        "                    for j in range(len(next_deltas))\n",
        "                )\n",
        "                a = activations[l + 1][i]\n",
        "                dσ_dz = self.sigmoid_derivative(a)\n",
        "                δ = weighted_sum * dσ_dz\n",
        "                layer_delta.append(δ)\n",
        "                print(f\"h{l+1}{i+1}:\")\n",
        "                print(f\"  Weighted sum = {weighted_sum:.6f}\")\n",
        "                print(f\"  ∂σ/∂z = a*(1-a) = {a:.4f}*{1-a:.4f} = {dσ_dz:.4f}\")\n",
        "                print(f\"  δ = Sum * ∂σ/∂z = {weighted_sum:.6f} * {dσ_dz:.4f} = {δ:.6f}\")\n",
        "            layer_deltas.insert(0, layer_delta)\n",
        "\n",
        "        print(\"\\n[Input Gradients]\")\n",
        "        input_grads = []\n",
        "        for i in range(self.num_inputs):\n",
        "            grad = sum(\n",
        "                layer_deltas[0][j] * self.hidden_layers[0].neurons[j].weights[i]\n",
        "                for j in range(len(self.hidden_layers[0].neurons))\n",
        "            )\n",
        "            print(f\"∂E/∂x{i+1} = {' + '.join(f'{layer_deltas[0][j]:.6f}*{self.hidden_layers[0].neurons[j].weights[i]:.2f}' for j in range(len(self.hidden_layers[0].neurons)))} = {grad:.6f}\")\n",
        "            input_grads.append(grad)\n",
        "\n",
        "# Run the network\n",
        "nn = NeuralNetwork()\n",
        "inputs = list(map(float, input(\"\\nEnter input values (space-separated): \").split()))\n",
        "target = 1.0  # Fixed target\n",
        "print(\"\\n=== NEURAL NETWORK EXECUTION ===\")\n",
        "print(f\"Network Architecture: {nn.num_inputs}-{'-'.join(map(str, nn.neurons_per_hidden))}-{nn.num_outputs}\")\n",
        "print(f\"Input Values: {inputs}\")\n",
        "print(\"Using fixed target = 1.0 for backpropagation (ignoring actual output error)\")\n",
        "activations = nn.full_forward_pass(inputs)\n",
        "nn.full_backward_pass(activations, target)\n",
        "1\n"
      ]
    }
  ]
}